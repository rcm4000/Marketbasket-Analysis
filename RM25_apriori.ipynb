{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which file would you like to load?  example: Macys.csv  =Macys.csv\n",
      "What is your minimum support? example: 50% =  89\n",
      "What is your confidence level percentage? example: 50%   = 95\n",
      " \n",
      "*********************************** Association Rule using Apriori Algorithm **********************************\n",
      "Created by:  Rayon Myrie\n",
      "Email:  RM25@njit.edu\n",
      "Student Id:  Rayon Myrie\n",
      "CLASS:  CS634 - Data Mining\n",
      "There are  10 unique items in your dataset. We are checking for items with a minimum support of 70 %\n",
      "and minimum confidence of 80 %. The items that have a minimum support of  70 % are  ['Jeans' 'Tshirt' 'underwear']\n",
      " \n",
      " \n",
      "Table:\n",
      "col_0      count  support\n",
      "Jeans         10    100.0\n",
      "Tshirt        10    100.0\n",
      "underwear      9     90.0\n",
      " \n",
      " \n",
      " \n",
      "The combinations for this set are: \n",
      "           Jeans  Tshirt  underwear\n",
      "Jeans       10.0     8.0        5.0\n",
      "Tshirt       0.0    10.0        5.0\n",
      "underwear    0.0     0.0        9.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The support for the frequent pairs are:\n",
      "                     count  support\n",
      "Jeans,underwear        5.0     50.0\n",
      "Tshirt,underwear       5.0     50.0\n",
      "Jeans,Tshirt           8.0     80.0\n",
      "underwear,underwear    9.0     90.0\n",
      "Jeans,Jeans           10.0    100.0\n",
      "Tshirt,Tshirt         10.0    100.0\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:157: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The pairs that meet the minumum support are:\n",
      "\n",
      "\n",
      "                     count  support Min_Conf\n",
      "Jeans,Tshirt           8.0     80.0       95\n",
      "underwear,underwear    9.0     90.0       95\n",
      "Jeans,Jeans           10.0    100.0       95\n",
      "Tshirt,Tshirt         10.0    100.0       95\n",
      "\n",
      "\n",
      "\n",
      "The Confidence for the pairs that meet the minumum support are:\n",
      "\n",
      "\n",
      "   Min_Conf  count_x  count_y     item_x               item_y  support_x  \\\n",
      "0        95       10     10.0      Jeans          Jeans,Jeans      100.0   \n",
      "1        95       10      8.0     Tshirt         Jeans,Tshirt      100.0   \n",
      "2        95       10     10.0     Tshirt        Tshirt,Tshirt      100.0   \n",
      "3        95        9      9.0  underwear  underwear,underwear       90.0   \n",
      "0        95       10      8.0      Jeans         Jeans,Tshirt      100.0   \n",
      "1        95       10     10.0      Jeans          Jeans,Jeans      100.0   \n",
      "2        95       10     10.0     Tshirt        Tshirt,Tshirt      100.0   \n",
      "3        95        9      9.0  underwear  underwear,underwear       90.0   \n",
      "\n",
      "   support_y  Confidence  \n",
      "0      100.0       100.0  \n",
      "1       80.0        80.0  \n",
      "2      100.0       100.0  \n",
      "3       90.0       100.0  \n",
      "0       80.0        80.0  \n",
      "1      100.0       100.0  \n",
      "2      100.0       100.0  \n",
      "3       90.0       100.0  \n",
      "\n",
      "\n",
      "The Items that meet the minimum Confidence are:\n",
      "\n",
      "\n",
      "   Min_Conf     item_x               item_y  support_y  Confidence\n",
      "0        95      Jeans          Jeans,Jeans      100.0       100.0\n",
      "2        95     Tshirt        Tshirt,Tshirt      100.0       100.0\n",
      "3        95  underwear  underwear,underwear       90.0       100.0\n",
      "1        95      Jeans          Jeans,Jeans      100.0       100.0\n",
      "2        95     Tshirt        Tshirt,Tshirt      100.0       100.0\n",
      "3        95  underwear  underwear,underwear       90.0       100.0\n",
      "\n",
      "\n",
      "The Most frequent combinations are:\n",
      "if the code was working correctly it would report the support and confidence for all the items below\n",
      "underwear\n",
      "Tshirt,underwear\n",
      "Jeans,Tshirt,underwear\n",
      "Jeans,Tshirt,underwear\n",
      "Jeans,Tshirt,underwear\n",
      "Jeans,Tshirt,underwear\n",
      "Jeans,Tshirt,underwear\n",
      "Jeans,Tshirt,underwear\n",
      "Jeans,Tshirt,underwear\n",
      "\n",
      " \n",
      "Graph\n",
      "Axes(0.125,0.125;0.62x0.755)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAAEoCAYAAAAZuzm8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRERhKbQREYWl0EZEFJZCGxFR2P8HqKBoPFJW39cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1f084c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from collections import OrderedDict\n",
    "from fractions import Fraction\n",
    "import difflib \n",
    "import io\n",
    "\n",
    "##enter percentage of minumum support\n",
    "\n",
    "## input data\n",
    "csvfilen =input('Which file would you like to load?  example: Macys.csv  =')\n",
    "min_support = input('What is your minimum support? example: 50% =  ')\n",
    "min_con=input('What is your confidence level percentage? example: 50%   = ')\n",
    "\n",
    "## read data from CSV file\n",
    "##min_supp = 70\n",
    "##min_con = 70\n",
    "##orditem = pd.read_csv('Macys.csv', names = ['order_no','item'])\n",
    "\n",
    "orditem = pd.read_csv(csvfilen, names = ['order_no','item'])\n",
    "\n",
    "\n",
    "##count of all items\n",
    "totitems=orditem['item'].nunique()\n",
    "## list of all the items\n",
    "countItem= pd.crosstab(index=orditem[\"item\"],  columns=\"count\").sort_index()\n",
    "\n",
    "\n",
    "##find support percent\n",
    "countItem['support']=(countItem['count']/totitems)*100\n",
    "\n",
    "##find  items above the minumum support\n",
    "sup2=countItem.loc[countItem['support'] > min_supp]\n",
    "\n",
    "##bar graph of all items\n",
    "plot_sup =orditem.item.value_counts().sort_index().plot('bar')\n",
    "\n",
    "## create basket\n",
    "cart = (orditem.groupby(['order_no','item'])['item'].count().unstack().reset_index().fillna(' ')\n",
    "          .set_index('order_no'))\n",
    "\n",
    "\n",
    "## save transposition of table\n",
    "cart.to_csv('cartMatrix.csv', sep=',')\n",
    "\n",
    "## create list of minumim Support items\n",
    "s=sup2.unstack().reset_index()\n",
    "\n",
    "s=s.drop(['col_0'], axis=1)\n",
    "\n",
    "s=s.drop_duplicates()\n",
    "\n",
    "u=s.item.unique()\n",
    "#u=u\n",
    "\n",
    "\n",
    "### Step two \n",
    "### reduce input file to only include the frequent Items\n",
    "red_ord =  pd.merge(orditem,s, on = ['item'], how = 'left').drop_duplicates(['order_no','item'])\n",
    "\n",
    "red_ord= red_ord.loc[red_ord[0] > 1]\n",
    "red_ord = red_ord.drop([0], axis=1)\n",
    "red_cart = (red_ord.groupby(['order_no','item'])['item'].count().unstack().reset_index().fillna(0).set_index('order_no'))\n",
    "##red_cart=red_cart.drop(['order_no'], axis=1)\n",
    "red_cart1 =red_ord.pivot(index=\"order_no\", columns=\"item\", values=\"item\").fillna(0)\n",
    "\n",
    "\n",
    "###########################\n",
    "###\n",
    "###   Test code\n",
    "##red_cart =pd.merge(red_cart,red_cart, how='left', left_on='order_no', right_on = 'order_no')\n",
    "\n",
    "##res = pd.merge(red_cart, red_cart, on='_tmpkey').drop('_tmpkey', axis=1)\n",
    "##res.index = pd.MultiIndex.from_product((red_cart.index, red_cart.index))\n",
    "##\n",
    "############################\n",
    "\n",
    "##Save matrix of Frequent Items to a CSV file for review and QA purposes(red_cart)\n",
    "red_cart.to_csv('Freq_matrix.csv', sep=',')\n",
    "#############################################\n",
    "\n",
    "\n",
    "#############################################\n",
    "##\n",
    "##  Find frequent Pairs\n",
    "## \n",
    "#############################################\n",
    "\n",
    "\n",
    "# take data matrix from dataframe\n",
    "redcart_matrix = red_cart.as_matrix()\n",
    "# get row and column Data\n",
    "rows, columns = redcart_matrix.shape\n",
    "redcart_frIt_matrix = np.zeros((columns,columns))\n",
    "##print(redcart_frIt_matrix )\n",
    "for c1_fr_set in range(0, 20):\n",
    "    for next_column in range(c1_fr_set , columns):\n",
    "        # multiply product pair vectors\n",
    "        product_vector = redcart_matrix[:,c1_fr_set] * redcart_matrix[:,next_column]\n",
    "        # check the number of pair occurrences in baskets\n",
    "        count_matches = sum((product_vector))\n",
    "        # save values to new matrix\n",
    "        redcart_frIt_matrix[c1_fr_set,next_column] = count_matches\n",
    "        frequent_items_df = pd.DataFrame(redcart_frIt_matrix, columns = red_cart.columns.values, index = red_cart.columns.values)\n",
    "\n",
    "##print(frequent_items_df)\n",
    "item = frequent_items_df.columns.values\n",
    "\n",
    "##print(item)\n",
    "#sb.heatmap(frequent_items_df)\n",
    "\n",
    "##############\n",
    "####  The orginal code for frequent pairs did not work, so I modifide a code I found that plotted a graph of frequent \n",
    "####  pairs.\n",
    "##############\n",
    "def extract_pairs(treshold):\n",
    "    output = {}\n",
    "    #select indexes with larger or equal n\n",
    "    matrix_coord_list = np.where(redcart_frIt_matrix >= treshold)\n",
    "    ##print(matrix_coord_list)\n",
    "    # take values\n",
    "    row_coords = matrix_coord_list[0]\n",
    "    column_coords = matrix_coord_list[1]  \n",
    "    #column_coord2 = matrix_coord_list[1] \n",
    "          \n",
    "    for index, value in enumerate(row_coords):\n",
    "            row = row_coords[index]\n",
    "            column = column_coords[index]\n",
    "          #  column2 = column_coords2[index]\n",
    "        # get product names\n",
    "            first_product = item[row]\n",
    "            second_product = item[column]\n",
    "            #third_product = item[column2]\n",
    "            #third_product = item[column2]\n",
    "            matches = redcart_frIt_matrix[row,column]\n",
    "            output[first_product+\",\"+second_product] = matches\n",
    "            sorted_output = OrderedDict(sorted(output.items(), key=lambda x: x[1]))\n",
    "            ##print  (matches)\n",
    "    return sorted_output  \n",
    "############################################################    \n",
    "print  (\" \")  \n",
    "\n",
    "n2 = pd.Series(extract_pairs(1))\n",
    "n2_frame =pd.DataFrame(data=n2, columns=['count'])\n",
    "n2_frame['support']=(n2_frame['count']/totitems)*100\n",
    "n2_frame_sup2=n2_frame.loc[n2_frame['support'] > min_supp]\n",
    "\n",
    "n2_frame_sup2.index.name= None\n",
    "sup2.index.name= None\n",
    "\n",
    "n2_frame_sup2['Min_Conf']=min_con\n",
    "n2_frame_sup2.to_csv('pair_support.csv', sep=',')  \n",
    "sup2.to_csv('indv_support.csv', sep=',')\n",
    "\n",
    "indiv_sup = pd.read_csv('indv_support.csv', names = ['item','count','support'],skiprows=[0])\n",
    "n2_support = pd.read_csv('pair_support.csv', names = ['item','count','support','Min_Conf'],skiprows=[0])\n",
    "### Add first 3 values to a new column as index\n",
    "n2_support['item1'] = n2_support['item'].str[:3]\n",
    "indiv_sup['item1'] = indiv_sup['item'].str[:3]\n",
    "### Add last 3 values to a new column as index\n",
    "n2_support['item2'] = n2_support['item'].str[-3:]\n",
    "indiv_sup['item2'] = indiv_sup['item'].str[-3:]\n",
    "##########\n",
    "### Merge dataframes to find the confidence ####\n",
    "mer_df1=pd.merge(indiv_sup,n2_support,how='left', left_on = 'item2', right_on = 'item2')\n",
    "mer_df2=pd.merge(indiv_sup,n2_support,how='left', left_on = 'item1', right_on = 'item1')\n",
    "mer_df3=pd.concat([mer_df1,mer_df2],)  \n",
    "#drop columns\n",
    "mer_df3=mer_df3.drop(['item1','item1_x','item1_y','item2','item2_x','item2_y'], axis=1)\n",
    "##mer_df3=mer_df3.drop_duplicates()\n",
    "##print(mer_df1)\n",
    "######################################\n",
    "###\n",
    "###   Calculate confidence\n",
    "###\n",
    "######################################\n",
    "mer_df3['Confidence']=(mer_df3['support_y']/mer_df3['support_x'])*100\n",
    "##print(mer_df3)\n",
    "\n",
    "##### find minumum confidence \n",
    "mer_df4=mer_df3.loc[mer_df3['Confidence'] > mer_df3['Min_Conf']]\n",
    "\n",
    "\n",
    "mer_df4=mer_df4.drop(['count_x','count_y', 'support_x'], axis=1)\n",
    "\n",
    "##print(n2_support)\n",
    "print('*********************************** Association Rule using Apriori Algorithm **********************************')\n",
    "\n",
    "###########################################################\n",
    "###\n",
    "###\n",
    "###  Print Statements\n",
    "###\n",
    "###\n",
    "###########################################################\n",
    "print('Created by:  Rayon Myrie')\n",
    "print('Email:  RM25@njit.edu')\n",
    "print('Student Id:  Rayon Myrie')\n",
    "print('CLASS:  CS634 - Data Mining')\n",
    "print(\"There are \", totitems, \"unique items in your dataset. We are checking for items with a minimum support of\",min_supp,'%') \n",
    "print(\"and minimum confidence of\", min_confid,\"%. The items that have a minimum support of \", min_supp,\"% are \",u)\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('Table:')\n",
    "print(sup2)\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('The combinations for this set are: ')\n",
    "print(frequent_items_df)\n",
    "print ('')\n",
    "print ('')\n",
    "print ('')\n",
    "print ('')\n",
    "print('The support for the frequent pairs are:')\n",
    "print(n2_frame)\n",
    "print ('')\n",
    "print ('')\n",
    "print ('')\n",
    "print ('')\n",
    "\n",
    "print('The pairs that meet the minumum support are:')\n",
    "print ('')\n",
    "print ('')\n",
    "print(n2_frame_sup2)\n",
    "print ('')\n",
    "print ('')\n",
    "print ('')\n",
    "print('The Confidence for the pairs that meet the minumum support are:')\n",
    "print ('')\n",
    "print ('')\n",
    "print(mer_df3)\n",
    "print ('')\n",
    "print ('')\n",
    "print('The Items that meet the minimum Confidence are:')\n",
    "print ('')\n",
    "print ('')\n",
    "print(mer_df4)\n",
    "print ('')\n",
    "print ('')\n",
    "\n",
    "######################################################################\n",
    "print ('The Most frequent combinations are:')\n",
    "print ('if the code was working correctly it would report the support and confidence for all the items below')\n",
    "#####################################################################\n",
    "#####  old code ------ Does not work- it finds the frequent pairs but does not give an acurate count.\n",
    "####                   I used this coulde for testing purposes.  This would be the best code, if \n",
    "###                    count and return feature was working propurely.\n",
    "###    find pairs of sets N >2 using Cross\n",
    "##     create sets for minumum supported items in basket\n",
    "##     find pairs using Naive approach\n",
    "######## Print#####################################################################\n",
    "\n",
    "c1frset = []\n",
    "for L in range(1, 10):\n",
    "    for c1frset in list(itertools.combinations(red_cart, L)):\n",
    "        freq1 = ','.join(c1frset)\n",
    "        freq=pd.Series(freq1)\n",
    "        freq=pd.DataFrame(data=freq)\n",
    "    print (freq1)\n",
    "print ('')    \n",
    "\n",
    "#print (pd.Series(orted_output))\n",
    "#plot_sup2 =frequent_items_df.support.sort_index().plot('bar')\n",
    "plot_sup2 =sns.heatmap(frequent_items_df)\n",
    "print(' ')\n",
    "print('Graph')\n",
    "print(plot_sup2) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
